{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesco Store finder\n",
    "\n",
    "The aim of this notebook is to collect the various pieces of information for all of the Tesco stores in the UK\n",
    "\n",
    "\n",
    "Approach\n",
    "\n",
    "If you use the Tesco store locator website ( https://www.tesco.com/store-locator/uk/ )you can find a list of local (you specify the locality) tesco stores. Each of which has its own web page and included in the web page is detailed information about the store. \n",
    "\n",
    "When you go to one of the store web pages you will notice that the URL is something like this:\n",
    "\n",
    "\n",
    "https://www.tesco.com/store-locator/uk/?bid=4634\n",
    "\n",
    "\n",
    "Internally Tesco are using four digit numbers to identify their stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are real stores  \n",
    "https://www.tesco.com/store-locator/uk/?bid=4634\n",
    "\n",
    "https://www.tesco.com/store-locator/uk/?bid=6367\n",
    "\n",
    "\n",
    "This one isn't\n",
    "\n",
    "\n",
    "https://www.tesco.com/store-locator/uk/?bid=9999\n",
    "\n",
    "\n",
    "There are about ~2500 Tesco stores inthe UK so you can see that a lot of the number range  1000 - 9999 will not actually represent a real store.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Browser Inspector\n",
    "\n",
    "All modern browsers allow you to access the underlying HTML Code which makes up a Web page\n",
    "\n",
    "It is the job of the Browser to interpret the HTML and present the information it represents on the screen in a user friendlt manner.\n",
    "\n",
    "In order to Web scrape, you do need to have some understanding of HTML but not a great deal. Like most coding languages it is easier to read than to write and we only need to be able to read it a little bit, e.g. recognise different components or tags and a bit about the syntax of tags. \n",
    "\n",
    "A more important requirement is to be able to match what we see on the screen with the underlying HTML. A thorough understanding of the HTML and CSS code will allow you to do this, but there is a far easier way.\n",
    "\n",
    "This involves using the developer tools found in all modern browsers and in particular the 'element inspector'. This allows you to select an element on the web page; a table, part of a table, a link, almost anything and have the corresponding HTML code highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information that we might want to scrape\n",
    "\n",
    "\n",
    "* Store Name\n",
    "* Store Address\n",
    "* Store Geo-location \n",
    "* store type\n",
    "* Store Post Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs      # pip install beautifulsoup4 but import from bs4\n",
    "import time\n",
    "import folium                            # !pip install folium - not included with the Anaconda python, so you may need to install\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'get' methods from requests only needs to be given a string representing a url\n",
    "\n",
    "Quite often if you need to provide multiple parameters you would build the url string up and then call  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick example to show how Beautifulsoup works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.bbc.co.uk')\n",
    "##print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can make the output look a bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text)               \n",
    "prettyHTML = soup.prettify() \n",
    "#print(prettyHTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can find all of the images within the Web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagelink in soup.find_all('img'):\n",
    "    url = imagelink.get('src')\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can find all of the links within the Web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = soup.findAll('a')\n",
    "for url in urls :\n",
    "    print(url.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `find` and `FindAll` allow you to search for tags\n",
    "\n",
    "## `get` allows you to select parameter values or tag values\n",
    "\n",
    "## Typically we will be finding tags and then extracting values from them.\n",
    "\n",
    "## What we need to ensure when doing this is that we have selected the correct tags. In any given webpage some of the common tags will occur many times as we have just seen with the 'a' tag.\n",
    "\n",
    "## we can do this by either using a chain of tags which is unique and ends in the tag we want or make use of the parameters and values within a tag and find a unique combination which will identify the specific tag we want.\n",
    "\n",
    "## This is why we need to inspect the HTML in order to identify these unique combinations.\n",
    "\n",
    "## In HTML tags are written in a specific way \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.tesco.com/store-locator/uk/?bid=6367')\n",
    "soup = bs(r.text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h1 in soup.find_all('h1'):\n",
    "    store_id = h1.get('title')\n",
    "    print(h1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Store Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h1 in soup.findAll('h1'):\n",
    "    store_id = h1.get('title')\n",
    "    print(store_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h2 in soup.find_all('h2'):\n",
    "    if h2.text == 'Address' :\n",
    "        print(h2.findNext('span').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Address\n",
    "address = soup.find('div', {'class': 'address'}).find('span', {'itemprop': 'streetAddress'} ).text\n",
    "print(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the class makes the element unique, I can use it.\n",
    "address = soup.find('span', {'itemprop': 'streetAddress'} )\n",
    "print(address.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As long as you uniquely identify the tag you want, how you get there generally doesn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Longitude and Latitude\n",
    "\n",
    "## This is a bit more involved and includes some simple python code to extract the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latitude and Longitude\n",
    "imagelink = soup.find('img')\n",
    "url = imagelink.get('src')\n",
    "\n",
    "item_list = url.split('/')\n",
    "lat = item_list[8].split(',')[0]\n",
    "lng = item_list[8].split(',')[1]\n",
    "\n",
    "print ('lat =', lat, 'lng =', lng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That is all of the bits of information we wanted to collect - that is the Web scraping done\n",
    "\n",
    "## So now we will put it all together and add a few python bits to accumulate the data from many stores in a single save dataset.\n",
    "\n",
    "\n",
    "## We need to :\n",
    "\n",
    "1. Make use of the file naming convention to loop through a large number of possible stores, accepting that some won't exist.\n",
    "2. Although wasteful of space we will save all of the 'requested' files seperately and then process them with Beautifulsoup\n",
    "3. Create a CSV file of all of the data we extract from the files using Beautifulsoup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1  --- DO NOT RUN\n",
    "\n",
    "stem = 'https://www.tesco.com/store-locator/uk/?bid='                      \n",
    "filename_prefix = './stores/'\n",
    "filename_suffix = '.html'\n",
    "for x in range(3000,4000) :\n",
    "    r = requests.get(stem + str(x))\n",
    "    filename = filename_prefix + str(x) + filename_suffix\n",
    "    f = open(filename, \"w\")\n",
    "    f.write(r.text)\n",
    "    f.close()\n",
    "    time.sleep(5)                          # explain why this is included  - 1000 store with 5 sec wait = 5000+ secs to run\n",
    "                                           # the wait is added as a courtesy so as not to overload the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given that we expect some of the values we used not to be actual stores we need to know how to identify a 'missing' store\n",
    "\n",
    "### In fact all calls return an HTML file, so we check to see which include 'Error' in the title.\n",
    "###  \n",
    "### In other scenarios the resuests call coul return a status_code of 404 (file not found) which you could check for using the `status_code` value which is included as part of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = requests.get('https://www.tesco.com/store-locator/uk/?bid=9999')\n",
    "soup = bs(rm.text)               \n",
    "#prettyHTML = soup.prettify() \n",
    "#print(prettyHTML)\n",
    "title = soup.find('title')\n",
    "print(title.text)\n",
    "if title.text[0:5] == 'Error' :\n",
    "    print(\"Ignore me\")\n",
    "else :\n",
    "    print(\"process me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have all of the files and can identify the 'duds'\n",
    "\n",
    "## We are ready to do the scraping\n",
    "\n",
    "## You could combine this step with the previous and do the scraping as you request the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "\n",
    "# create the dataframe\n",
    "store_info = pd.DataFrame(columns = ['Id', 'Name', 'Address', 'lat', 'lng'])\n",
    "\n",
    "# read the data from a file\n",
    "folder  = './stores/'\n",
    "for i in range(3000,4000) :\n",
    "    print(i)\n",
    "    filename = folder + str(i) + '.html'    \n",
    "    with open(filename, 'r') as file_handle: content = file_handle.read()\n",
    "    soup = bs(content) \n",
    "    title = soup.find('title')\n",
    "    if title.text[0:5] != 'Error' :\n",
    "        h1 = soup.find('h1')\n",
    "        store_id = h1.get('title')\n",
    "        store_name = h1.text\n",
    "        address = soup.find('div', {'class': 'address'}).find('span', {'itemprop': 'streetAddress'} ).text\n",
    "        imagelink = soup.find('img')\n",
    "        url = imagelink.get('src')\n",
    "        item_list = url.split('/')\n",
    "        lat = item_list[8].split(',')[0]\n",
    "        lng = item_list[8].split(',')[1]\n",
    "        #print(store_id + \" ++ \" + store_name + \" ++ \" + address + \" ++ \" + lat + \" ++ \" + lng)\n",
    "        store_info = store_info.append({'Id' : store_id, 'Name' : store_name, \"Address\" : address, \"lat\" : lat, \"lng\" : lng}, ignore_index=True)\n",
    "print(store_info.shape)\n",
    "print(store_info.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The store type is part of the store name and the post code is included in the address, so extracting them is just Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "### Adding the store_type and post_code columns\n",
    "\n",
    "def last_element(s, split_on) :\n",
    "    l = s.split(split_on)\n",
    "    return l[len(l) - 1]\n",
    "\n",
    "store_info['store_type'] = store_info.apply(lambda row: last_element(row.Name, ' '), axis=1)\n",
    "store_info['post_code'] = store_info.apply(lambda row: last_element(row.Address, ','), axis=1)\n",
    "store_info.head()\n",
    "store_info.to_csv('xstore_info.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have our file of data, lets put it on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('store_info.csv')\n",
    "uk = folium.Map(location=[53, -1], control_scale=True, zoom_start=7)\n",
    "\n",
    "\n",
    "# adding the markers and pop ups\n",
    "for i in range(0,len(data)):\n",
    "    popup_data = data.iloc[i]['post_code'] +'\\n' + data.iloc[i]['store_type']\n",
    "    folium.Marker([ data.iloc[i]['lat'], data.iloc[i]['lng']], popup=popup_data).add_to(uk)\n",
    "uk\n",
    "#uk.save('Tesco_stores.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
